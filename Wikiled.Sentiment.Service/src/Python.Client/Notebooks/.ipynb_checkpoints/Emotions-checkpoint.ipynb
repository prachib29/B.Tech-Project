{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8a2511e66cfe>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8a2511e66cfe>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    logFormatter .= logging.Formatter('%(asctime)s - [%(thread)s] [%(threadName)s]- %(name)s - %(levelname)s - %(message)s')\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# !pip install nest_asyncio\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "import logging.config\n",
    "from psenti import SentimentAnalysis, SentimentConnection, Document\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import socket\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "user_name = socket.gethostname()\n",
    "host = 'sentiment2.wikiled.com'\n",
    "port = 80\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "nest_asyncio.apply(loop=asyncio.get_event_loop())\n",
    "\n",
    "logger = logging.getLogger('JupyterUI')\n",
    "logFormatter = logging.Formatter('%(asctime)s - [%(thread)s] [%(threadName)s]- %(name)s - %(levelname)s - %(message)s')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(logFormatter)\n",
    "console.setLevel(logging.DEBUG)\n",
    "\n",
    "logger.addHandler(console)\n",
    "\n",
    "connection = SentimentConnection(host=host, port=port, client_id=user_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup results Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['Huge profit reported.', 'Huge loss reported.', 'This stock is performing poor.', 'I am not going to invest in this stock.', \n",
    "           'I will invest in this stock.', 'putting apple record-breaking quarter into context: $aapl URL_URL via @forbestech']\n",
    "# reviews = ['I love this hello kitty decal! I like that the bow is pink instead of red. Only bad thing is that after putting it on the window theres a few air bubbles, but that most likely my fault. Shipped fast too.', \n",
    "#            'This is the worst scenario case.']\n",
    "SentimentStars = []\n",
    "TotalSentimentWords = []\n",
    "TotalWords = []\n",
    "Emot= []\n",
    "for review in reviews:\n",
    "    TotalWords.append(len(list(review.split(' '))))\n",
    "def process_result(result):    \n",
    "#     if 'Attributes' in result:\n",
    "#         for key, value in result['Attributes'].items():\n",
    "#             logger.info(f'Document emotion [{key}]: {float(value):1.2f}')\n",
    "    SentimentStars.append(result[\"Stars\"])\n",
    "#     logger.info('=' * 20)    \n",
    "#     logger.info('Emotion Words:')    \n",
    "#     logger.info('=' * 20)\n",
    "    count = 0;\n",
    "    myMap = dict({'Anger' : 0, 'Anticipation' : 0, 'Disgust' : 0, 'Fear' : 0, 'Joy' : 0, 'Sadness' : 0, 'Surprise' : 0, 'Trust': 0})\n",
    "    for sentence in result['Sentences']:\n",
    "        for word in sentence['Words']:\n",
    "            if 'Emotions' in word and word['Emotions'] != ['None']:\n",
    "                count = count + 1;\n",
    "                for emotion in word['Emotions']:\n",
    "                    myMap[emotion] = myMap[emotion] + 1;\n",
    "    Emot.append(myMap)\n",
    "                \n",
    "    TotalSentimentWords.append(count)\n",
    "#     logger.info('=' * 20)    \n",
    "#     logger.info('Word Attributes:')    \n",
    "#     logger.info('=' * 20)    \n",
    "#     for sentence in result['Sentences']:\n",
    "#         for word in sentence['Words']:            \n",
    "#             if 'Attributes' in word and len(word['Attributes']) > 0:\n",
    "#                 logger.info(f\"Word [{word['Text']}] Attributes: {word['Attributes']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    l = []\n",
    "    l.append(review)\n",
    "    analysis = SentimentAnalysis(connection, extract_emotions=True)\n",
    "    analysis.on_message.subscribe(lambda result: process_result(result))\n",
    "    analysis.detect_sentiment_text(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentStars</th>\n",
       "      <th>TotalSentimentWords</th>\n",
       "      <th>TotalWords</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentimentStars  TotalSentimentWords  TotalWords  Anger  Anticipation  \\\n",
       "0             5.0                    0           3      0             0   \n",
       "1             1.0                    1           3      1             0   \n",
       "2             1.0                    0           5      0             0   \n",
       "3             1.0                    0           9      0             0   \n",
       "4             NaN                    0           6      0             0   \n",
       "5             NaN                    0          10      0             0   \n",
       "\n",
       "   Disgust  Fear  Joy  Sadness  Surprise  Trust  \n",
       "0        0     0    0        0         0      0  \n",
       "1        0     1    0        1         0      0  \n",
       "2        0     0    0        0         0      0  \n",
       "3        0     0    0        0         0      0  \n",
       "4        0     0    0        0         0      0  \n",
       "5        0     0    0        0         0      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'SentimentStars' : SentimentStars, 'TotalSentimentWords' : TotalSentimentWords, 'TotalWords' : TotalWords})\n",
    "df1 = pd.DataFrame(Emot)\n",
    "# print(df1)\n",
    "df = pd.concat([df, df1], sort=False, axis =1)\n",
    "# df.to_csv (r'/home/prachi/Wikiled.Sentiment.Service/csv/testing.csv', index = False, header=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/prachi/Wikiled.Sentiment.Service/lexicons/finance.json') as f:\n",
    "#     lexicon = json.loads(f)\n",
    "with open('/home/prachi/Wikiled.Sentiment.Service/lexicons/twitter.json', 'r') as f:\n",
    "    lexicon = json.loads(f.read())\n",
    "for review in reviews:\n",
    "    l = []\n",
    "    l.append(review)\n",
    "    analysis = SentimentAnalysis(connection, extract_emotions=True, lexicon= lexicon, domain='market')\n",
    "    analysis.on_message.subscribe(lambda result: process_result(result))\n",
    "    analysis.detect_sentiment_text(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.0, 1.0, 1.0, 1.0, None, None, None, None, 1.0, 3.0, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentStars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 5, 9, 6, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TotalWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-749c58fe961b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'SentimentStars'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mSentimentStars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TotalSentimentWords'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTotalSentimentWords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TotalWords'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTotalWords\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(df1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# df.to_csv (r'/home/prachi/Wikiled.Sentiment.Service/csv/testing.csv', index = False, header=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         ]\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'SentimentStars' : SentimentStars, 'TotalSentimentWords' : TotalSentimentWords, 'TotalWords' : TotalWords})\n",
    "df1 = pd.DataFrame(Emot)\n",
    "# print(df1)\n",
    "df = pd.concat([df, df1], sort=False, axis =1)\n",
    "# df.to_csv (r'/home/prachi/Wikiled.Sentiment.Service/csv/testing.csv', index = False, header=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv (r'/home/prachi/PycharmProjects/pythonProject/tesla_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>$TSLA\\n Tesla is running out of FOMO buyers......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA\\n Chris Wallace condemns Trump claims th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>$TSLA\\n  good time to add more? or is there a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>$TSLA\\n -&gt; Senate control in flux as counting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>$TSLA\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>$QQQ\\n $SPY\\n $AAPL\\n $MSFT\\n $TSLA\\n  … why o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>$TSLA\\n man crazy! Trump got lead back in TX a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>$SPY\\n  $TTCF\\n $FSR\\n $TSLA\\n $IPOB\\n Texas s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>$SPY\\n $TSLA\\n Nasdaq futures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>$TSLA\\n nasdaq going to the moon whole dow and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Comments\n",
       "0    $TSLA\\n Tesla is running out of FOMO buyers......\n",
       "1    $TSLA\\n Chris Wallace condemns Trump claims th...\n",
       "2    $TSLA\\n  good time to add more? or is there a ...\n",
       "3    $TSLA\\n -> Senate control in flux as counting ...\n",
       "4                                             $TSLA\\n \n",
       "..                                                 ...\n",
       "495  $QQQ\\n $SPY\\n $AAPL\\n $MSFT\\n $TSLA\\n  … why o...\n",
       "496  $TSLA\\n man crazy! Trump got lead back in TX a...\n",
       "497  $SPY\\n  $TTCF\\n $FSR\\n $TSLA\\n $IPOB\\n Texas s...\n",
       "498                      $SPY\\n $TSLA\\n Nasdaq futures\n",
       "499  $TSLA\\n nasdaq going to the moon whole dow and...\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentStars = []\n",
    "TotalSentimentWords = []\n",
    "TotalWords = []\n",
    "Emot= []\n",
    "for i in range(500):\n",
    "    TotalWords.append(len(list(df1['Comments'][i].split(' '))))\n",
    "def process_result(result):    \n",
    "    SentimentStars.append(result[\"Stars\"])\n",
    "    count = 0;\n",
    "    myMap = dict({'Anger' : 0, 'Anticipation' : 0, 'Disgust' : 0, 'Fear' : 0, 'Joy' : 0, 'Sadness' : 0, 'Surprise' : 0, 'Trust': 0})\n",
    "    for sentence in result['Sentences']:\n",
    "        for word in sentence['Words']:\n",
    "            if 'Emotions' in word and word['Emotions'] != ['None']:\n",
    "                count = count + 1;\n",
    "                for emotion in word['Emotions']:\n",
    "                    myMap[emotion] = myMap[emotion] + 1;\n",
    "    Emot.append(myMap)\n",
    "                \n",
    "    TotalSentimentWords.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    l = []\n",
    "    l.append(df1['Comments'][i])\n",
    "    analysis = SentimentAnalysis(connection, extract_emotions=True)\n",
    "    analysis.on_message.subscribe(lambda result: process_result(result))\n",
    "    analysis.detect_sentiment_text(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'SentimentStars' : SentimentStars, 'TotalSentimentWords' : TotalSentimentWords, 'TotalWords' : TotalWords})\n",
    "df3 = pd.DataFrame(Emot)\n",
    "df2 = pd.concat([df1, df2, df3], sort=False, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(r'/home/prachi/PycharmProjects/pythonProject/tesla_new_results.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
